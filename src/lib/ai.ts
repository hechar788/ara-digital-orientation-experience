'use server'

import OpenAI, { APIError } from 'openai'
import type { Response as OpenAIResponse, ResponseCreateParamsNonStreaming } from 'openai/resources/responses/responses'
import { matchLocationByKeywords, VALID_LOCATION_ID_SET, getFinalOrientation } from './ai.locations'
import { buildSystemPrompt, NAVIGATION_TOOL, DOCUMENTS_TOOL, AVAILABLE_LOCATIONS_TOOL, SUMMARISATION_SYSTEM_PROMPT } from './ai.prompts'
import { findPath, getRouteDescription, validatePath } from './pathfinding'

/**
 * Maps virtual location IDs to their actual photo IDs for navigation
 *
 * Some locations in the vector store use virtual IDs to distinguish between
 * different facilities at the same photo location (e.g., Visions Restaurant
 * and The Pantry both at outside-s-east-5).
 */
const VIRTUAL_LOCATION_MAP: Record<string, string> = {
  'outside-s-east-5-visions': 'outside-s-east-5',
  'outside-s-east-5-pantry': 'outside-s-east-5',
  'x-f1-west-10-finance': 'x-f1-west-10',
  'x-f1-west-10-careers': 'x-f1-west-10',
  'library-f1-6-pod': 'library-f1-6',
  's-f1-south-2-s154': 's-f1-south-2',
  's-f1-south-2-s156': 's-f1-south-2',
  's-f2-south-5-s265': 's-f2-south-5',
  's-f2-south-5-s254': 's-f2-south-5',
  's-f2-south-7-s256': 's-f2-south-7',
  's-f2-south-7-s264': 's-f2-south-7',
  's-f2-south-7-s262': 's-f2-south-7',
  's-f4-north-8-s454': 's-f4-north-8',
  's-f4-north-8-s455': 's-f4-north-8',
  'x-f2-north-9-aside-l202': 'x-f2-north-9-aside',
  'x-f2-north-9-aside-l203': 'x-f2-north-9-aside',
  'x-f2-west-3-aside-x203': 'x-f2-west-3-aside',
  'x-f2-west-4-x204': 'x-f2-west-4',
  'x-f2-west-4-x206': 'x-f2-west-4',
  'x-f2-west-5-aside-x205': 'x-f2-west-5-aside',
  'x-f2-west-5-aside-x210': 'x-f2-west-5-aside',
  'x-f3-east-6-x306': 'x-f3-mid-4',
  'x-f3-east-6-x308': 'x-f3-mid-4',
  'x-f3-west-1-x302': 'x-f3-west-1',
  'x-f3-west-1-x304': 'x-f3-west-1',
  'x-f3-west-1-aside-x303': 'x-f3-west-1-aside',
  'x-f3-west-1-aside-x305': 'x-f3-west-1-aside'
}

/**
 * Resolves virtual location IDs to their actual photo IDs for navigation
 *
 * @param photoId - Location ID that may be virtual or real
 * @returns Actual photo ID for navigation, or the original ID if not virtual
 */
function resolvePhotoId(photoId: string): string {
  return VIRTUAL_LOCATION_MAP[photoId] ?? photoId
}

let cachedClient: OpenAI | null = null
type GeneratedResponse = OpenAIResponse
type CreateResponseResult = Awaited<ReturnType<OpenAI['responses']['create']>>

function getOpenAIClient(): OpenAI {
  if (!cachedClient) {
    const apiKey = process.env.OPENAI_API_KEY

    if (!apiKey) {
      throw new Error('OPENAI_API_KEY is not set. Add it to your environment before using getChatResponse().')
    }

    cachedClient = new OpenAI({
      apiKey
    })
  }

  return cachedClient
}

function getVectorStoreId(): string {
  const vectorStoreId = process.env.OPENAI_LOCATIONS_VECTOR_STORE_ID

  if (!vectorStoreId) {
    throw new Error('OPENAI_LOCATIONS_VECTOR_STORE_ID is not set. Add it to your environment before using getChatResponse().')
  }

  return vectorStoreId
}

function isNonStreamingResponse(response: CreateResponseResult): response is GeneratedResponse {
  return typeof response === 'object' && response !== null && 'output' in response
}

function ensureNonStreamingResponse(response: CreateResponseResult): GeneratedResponse {
  if (isNonStreamingResponse(response)) {
    return response
  }
  throw new Error('OpenAI returned a streaming response; expected non-streaming output.')
}

/**
 * Describes a chat message exchanged with the AI assistant
 *
 * Used to pass prior conversation history back to the Responses API so the model
 * can reply with awareness of previous turns.
 *
 * @property role - Sender role for the message (`user`, `assistant`, or `system`)
 * @property content - Message body text provided to the model
 *
 * @example
 * ```typescript
 * const message: ChatMessage = { role: 'user', content: 'Where is the cafe?' }
 * ```
 */
export interface ChatMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

/**
 * Represents a function call generated by the AI assistant
 *
 * Extended to support both navigation and document display functions. Navigation
 * function calls include pathfinding metadata for step-by-step routes, while
 * document function calls trigger the campus documents popup.
 *
 * @property name - Function identifier returned by the model (`navigate_to` or `show_campus_documents`)
 * @property arguments - Function-specific payload supplied with the call
 *
 * @example
 * ```typescript
 * // Navigation function call
 * const navCall: FunctionCall = {
 *   name: 'navigate_to',
 *   arguments: {
 *     photoId: 'library-f1-entrance',
 *     path: ['a-f1-north-entrance', 'a-f1-hallway', 'library-f1-entrance'],
 *     distance: 2,
 *     routeDescription: 'Route found: 2 steps from A Block F1 (North Entrance) to Library F1 Entrance.'
 *   }
 * }
 * 
 * // Documents function call
 * const docsCall: FunctionCall = {
 *   name: 'show_campus_documents',
 *   arguments: {}
 * }
 * ```
 */
export type FunctionCall = 
  | {
      name: 'navigate_to'
      arguments: {
        photoId: string
        path?: string[]
        distance?: number
        routeDescription?: string
        finalOrientation?: number
        error?: string
      }
    }
  | {
      name: 'show_campus_documents'
      arguments: Record<string, never>
    }
  | {
      name: 'show_available_locations'
      arguments: Record<string, never>
    }

/**
 * Structured response returned from getChatResponse
 *
 * Used by the UI to differentiate between plain text replies, navigation tool
 * invocations, and recoverable errors.
 *
 * @property message - Assistant text reply or `null` when an error occurs
 * @property functionCall - Navigation command selected by the AI assistant
 * @property error - Present when the request fails for any reason
 *
 * @example
 * ```typescript
 * const response: ChatResponse = {
 *   message: 'Head through the atrium, then turn left toward the library entrance.',
 *   functionCall: {
 *     name: 'navigate_to',
 *     arguments: {
 *       photoId: 'library-f1-entrance',
 *       path: ['a-f1-north-entrance', 'a-f1-hallway', 'library-f1-entrance'],
 *       distance: 2,
 *       routeDescription: 'Route found: 2 steps from A Block F1 (North Entrance) to Library F1 Entrance.'
 *     }
 *   }
 * }
 * ```
 */
export interface ChatResponse {
  message: string | null
  functionCall: FunctionCall | null
  error?: string
}

/**
 * Maintains condensed chat history that survives across requests
 *
 * Tracks the rolling summary generated by the AI alongside the most recent
 * granular exchanges so future turns can reuse prior context without sending
 * the full transcript.
 *
 * @property summary - Condensed narrative of the conversation so far or `null` when no summary exists
 * @property messages - Recent verbatim chat messages preserved for precise follow-up handling
 *
 * @example
 * ```typescript
 * const state: ConversationState = {
 *   summary: 'Goal: Locate the library. Confirmed: navigation requested to library-f1-entrance.',
 *   messages: [{ role: 'user', content: 'Can you take me to the library?' }]
 * }
 * ```
 */
export interface ConversationState {
  summary: string | null
  messages: ChatMessage[]
}

/**
 * Payload accepted by executeChatWithSummaries
 *
 * Carries the conversation memory from the client together with the next user
 * message and the viewer's current location identifier.
 *
 * @property state - Previously returned conversation state or `null` when the chat is new
 * @property nextMessage - Next chat turn to process, typically the latest user input
 * @property currentLocation - Viewer photo identifier describing the user's position
 *
 * @example
 * ```typescript
 * const input: ExecuteChatWithSummariesInput = {
 *   state: { summary: null, messages: [] },
 *   nextMessage: { role: 'user', content: 'Where is the cafeteria?' },
 *   currentLocation: 'a-f1-north-entrance'
 * }
 * ```
 */
export interface ExecuteChatWithSummariesInput {
  state: ConversationState | null
  nextMessage: ChatMessage
  currentLocation: string
}

/**
 * Result structure returned by executeChatWithSummaries
 *
 * Bundles the assistant response with the updated conversation memory that the
 * client should persist for the next request.
 *
 * @property response - Chat output including optional navigation tool call
 * @property state - Conversation state reflecting any new summaries and trimmed history
 *
 * @example
 * ```typescript
 * const result = await executeChatWithSummaries({
 *   state: { summary: null, messages: [] },
 *   nextMessage: { role: 'user', content: 'Take me to the gym' },
 *   currentLocation: 'a-f1-north-entrance'
 * })
 * console.log(result.state.summary)
 * // "Goal: Reach the gym. Confirmed navigation requested."
 * ```
 */
export interface ExecuteChatWithSummariesResult {
  response: ChatResponse
  state: ConversationState
}

const MAX_MESSAGE_COUNT = 20
const MAX_TOTAL_CHARACTERS = 5000
const SUMMARY_TRIGGER_THRESHOLD = 10
const SUMMARY_CHARACTER_THRESHOLD = 3500
const SUMMARY_TAIL_MESSAGE_COUNT = 8
const NAVIGATION_PROMPT_PHRASES = [
  'would you like me to show you how to get there',
  'would you like directions',
  'would you like me to guide you',
  'shall i show you'
]
const DOCUMENTS_PROMPT_PHRASES = [
  'would you like me to pull it up for you',
  'would you like me to show you the documents',
  'would you like to see the documents',
  'shall i show you the documents'
]
const AVAILABLE_LOCATIONS_REQUEST_PATTERNS = [
  'what locations can you take me to',
  'what locations can you help me find',
  'what locations can you show me',
  'what locations do you have',
  'what facilities can you take me to',
  'what facilities can you help me find',
  'what classrooms can you take me to',
  'what classrooms can you help me find',
  'show me the locations you can take me to',
  'show me the locations you can help me find',
  'show me available locations',
  'list available locations',
  'where can you take me',
  'where can you take me?',
  'list the locations you can take me to',
  'list the locations you can help me find'
]
const AFFIRMATIVE_SINGLE_WORDS = [
  'yes',
  'yeah',
  'yep',
  'yup',
  'sure',
  'ok',
  'okay',
  'please',
  'absolutely',
  'definitely'
]
const AFFIRMATIVE_MULTI_WORD_PHRASES = ['go ahead', 'take me', 'show me', 'sounds good', 'do it']

function normaliseMessageForMatching(message: string): string {
  return message
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, ' ')
    .replace(/\s+/g, ' ')
    .trim()
}

function isAvailableLocationsRequest(message: string): boolean {
  const normalised = normaliseMessageForMatching(message)
  if (!normalised) {
    return false
  }

  if (AVAILABLE_LOCATIONS_REQUEST_PATTERNS.some(pattern => normalised.includes(pattern))) {
    return true
  }

  const hasLocationKeyword = /\b(locations?|classrooms?|facilities?)\b/.test(normalised)
  if (!hasLocationKeyword) {
    return false
  }

  const hasListingIntent =
    /\b(available|list|which|what)\b/.test(normalised) || normalised.includes('show me')
  const hasAssistanceIntent =
    /\b(can you|help me|take me|show me|do you have|tell me)\b/.test(normalised)

  return hasListingIntent && hasAssistanceIntent
}

function parseResponseText(output: GeneratedResponse['output']): string | null {
  const parts: string[] = []
  for (const item of output ?? []) {
    if (item.type === 'message' && Array.isArray(item.content)) {
      for (const contentPart of item.content) {
        if (contentPart.type === 'output_text' && contentPart.text) {
          parts.push(contentPart.text)
        }
      }
    }
  }
  const combined = parts.join('').trim()
  return combined.length > 0 ? combined : null
}

function parseFunctionCall(output: GeneratedResponse['output']): FunctionCall | null {
  for (const item of output ?? []) {
    if (item.type === 'function_call') {
      if (item.name === 'navigate_to') {
        try {
          const parsedArguments = JSON.parse(item.arguments ?? '{}')
          const photoId = typeof parsedArguments.photoId === 'string' ? parsedArguments.photoId : undefined
          if (photoId && VALID_LOCATION_ID_SET.has(photoId)) {
            return {
              name: 'navigate_to',
              arguments: { photoId }
            }
          }
        } catch (error) {
          console.error('[AI] Failed to parse navigate_to function call arguments', error)
        }
      } else if (item.name === 'show_campus_documents') {
        return {
          name: 'show_campus_documents',
          arguments: {}
        }
      } else if (item.name === 'show_available_locations') {
        return {
          name: 'show_available_locations',
          arguments: {}
        }
      }
    }
  }
  return null
}

function normaliseMessageInput(messages: ChatMessage[]) {
  return messages.map(message => ({
    role: message.role,
    content: message.content,
    type: 'message' as const
  }))
}

function sanitiseAssistantMessage(text: string | null): string | null {
  if (!text) {
    return null
  }

  // Normalise line endings and collapse excessive newlines
  let processedText = text.replace(/\r\n/g, '\n').replace(/\n{3,}/g, '\n\n')

  // Remove incomplete sentences or fragments at the end of the text
  processedText = processedText.replace(/\s*\([^)]*\)?$/, '')

  // Remove markdown-style formatting (e.g., **, *)
  processedText = processedText.replace(/\*\*|\*/g, '')

  // Trim leading/trailing whitespace
  processedText = processedText.trim()

  return processedText
}

function isNavigationOffer(text: string | null | undefined): boolean {
  if (!text) {
    return false
  }
  const lowerCased = text.toLowerCase()
  return NAVIGATION_PROMPT_PHRASES.some(phrase => lowerCased.includes(phrase))
}

function isDocumentsOffer(text: string | null | undefined): boolean {
  if (!text) {
    return false
  }
  const lowerCased = text.toLowerCase()
  return DOCUMENTS_PROMPT_PHRASES.some(phrase => lowerCased.includes(phrase))
}

function isAffirmativeUserResponse(message: string): boolean {
  const normalised = message.trim().toLowerCase()
  if (!normalised) {
    return false
  }
  const cleaned = normalised.replace(/[.,!?]/g, ' ').replace(/\s+/g, ' ').trim()
  if (!cleaned) {
    return false
  }
  if (
    AFFIRMATIVE_SINGLE_WORDS.some(
      word => cleaned === word || cleaned.startsWith(`${word} `) || cleaned.endsWith(` ${word}`)
    )
  ) {
    return true
  }
  return AFFIRMATIVE_MULTI_WORD_PHRASES.some(
    phrase => cleaned === phrase || cleaned.includes(`${phrase} `) || cleaned.includes(` ${phrase}`) || cleaned.endsWith(phrase)
  )
}

function extractPendingDestinationId(
  lastAssistantMessage: ChatMessage | undefined,
  messages: ChatMessage[]
): string | null {
  if (!lastAssistantMessage?.content || !isNavigationOffer(lastAssistantMessage.content)) {
    return null
  }

  const candidateSources: string[] = [lastAssistantMessage.content]
  for (let index = messages.length - 1; index >= 0; index -= 1) {
    const message = messages[index]
    if (!message.content || message === lastAssistantMessage) {
      continue
    }
    if (message.role === 'user') {
      candidateSources.push(message.content)
    } else if (message.role === 'assistant' && isNavigationOffer(message.content)) {
      candidateSources.push(message.content)
    }
    if (candidateSources.length >= 5) {
      break
    }
  }

  for (const content of candidateSources) {
    const matchedPhotoId = matchLocationByKeywords(content)
    if (matchedPhotoId && VALID_LOCATION_ID_SET.has(matchedPhotoId)) {
      return matchedPhotoId
    }
  }

  return null
}

/**
 * Validates that a matched location exists and is routable
 *
 * Checks the vector store to ensure the location has proper metadata and is navigable.
 * Also verifies pathfinding data exists so we can actually route to this location.
 * This prevents the fallback system from making up information about non-existent locations.
 *
 * @param photoId - Location identifier to validate
 * @param currentLocation - Current user location for pathfinding validation
 * @returns Location metadata if valid and routable, null otherwise
 */
function validateAndGetLocationMetadata(photoId: string, currentLocation?: string): {
  roomNumbers: string[]
  areaName: string
  floorLevel: number
  buildingBlock: string
} | null {
  // First check if this is a valid location ID
  if (!VALID_LOCATION_ID_SET.has(photoId)) {
    console.warn('[AI] PhotoId not in valid location set', { photoId })
    return null
  }
  
  // Import vector store data
  const vectorStoreLocations = require('../data/locations-vector-store.json')
  
  if (!Array.isArray(vectorStoreLocations)) {
    console.error('[AI] Vector store data is malformed')
    return null
  }
  
  const location = vectorStoreLocations.find((loc: any) => loc.id === photoId)
  
  if (!location || !location.metadata) {
    console.warn('[AI] Location not found in vector store', { photoId })
    return null
  }
  
  const { metadata } = location
  
  // Validate required fields for routable classroom locations
  if (!metadata.roomNumbers || !metadata.areaName || metadata.floorLevel === undefined || !metadata.buildingBlock) {
    console.warn('[AI] Location missing required metadata fields', { 
      photoId,
      hasRoomNumbers: !!metadata.roomNumbers,
      hasAreaName: !!metadata.areaName,
      hasFloorLevel: metadata.floorLevel !== undefined,
      hasBuildingBlock: !!metadata.buildingBlock
    })
    return null
  }
  
  // If current location provided, validate pathfinding is possible
  if (currentLocation) {
    const resolvedDestination = resolvePhotoId(photoId)
    const pathResult = findPath(currentLocation, resolvedDestination)
    
    if (!pathResult || !validatePath(pathResult)) {
      console.warn('[AI] No valid path found to location', {
        from: currentLocation,
        to: photoId,
        resolved: resolvedDestination,
        hasPath: !!pathResult
      })
      return null
    }
  }
  
  return {
    roomNumbers: metadata.roomNumbers,
    areaName: metadata.areaName,
    floorLevel: metadata.floorLevel,
    buildingBlock: metadata.buildingBlock
  }
}

/**
 * Generates a natural fallback response when vector store search fails but keyword matching succeeds
 *
 * Extracts location metadata from the vector store JSON to create a helpful confirmation message
 * that maintains the two-step navigation workflow. Only generates fallback if location is validated
 * and pathfinding to the location is possible.
 *
 * @param photoId - Matched location identifier from keyword system
 * @param currentLocation - Current user location for pathfinding validation
 * @returns Natural language response offering navigation, or null if location cannot be validated
 */
function generateFallbackLocationResponse(photoId: string, currentLocation?: string): string | null {
  // Validate location exists and is routable using vector store data
  const locationData = validateAndGetLocationMetadata(photoId, currentLocation)
  
  if (!locationData) {
    return null
  }
  
  const { roomNumbers, buildingBlock, floorLevel } = locationData
  
  const floorNames: Record<number, string> = {
    1: 'first floor',
    2: 'second floor',
    3: 'third floor',
    4: 'fourth floor'
  }
  
  const floorText = floorNames[floorLevel] || `floor ${floorLevel}`
  const block = buildingBlock.toUpperCase()
  
  // Use the actual room number from vector store metadata
  if (roomNumbers && roomNumbers.length > 0) {
    const primaryRoom = roomNumbers[0]
    return `${primaryRoom} is located in ${block} Block on the ${floorText}.\n\nWould you like me to show you how to get there?`
  }
  
  return `This location is in ${block} Block on the ${floorText}.\n\nWould you like me to show you how to get there?`
}

function gatherOverrideSources(
  messages: ChatMessage[],
  assistantMessage: string | null
): Array<{ content: string; role: ChatMessage['role'] | 'assistant' }> {
  const sources: Array<{ content: string; role: ChatMessage['role'] | 'assistant' }> = []
  for (let index = messages.length - 1; index >= 0; index -= 1) {
    const message = messages[index]
    if (message.role === 'user' && message.content.trim().length > 0) {
      sources.push({ content: message.content, role: message.role })
    }
  }
  if (assistantMessage && assistantMessage.trim().length > 0) {
    sources.push({ content: assistantMessage, role: 'assistant' })
  }
  return sources
}

function applyKeywordOverrides(
  originalCall: FunctionCall | null,
  assistantMessage: string | null,
  messages: ChatMessage[],
  currentLocation?: string
): { call: FunctionCall | null; overridden: boolean; fallbackMessage?: string } {
  const sources = gatherOverrideSources(messages, assistantMessage)

  for (const source of sources) {
    const matchedPhotoId = matchLocationByKeywords(source.content)
    if (!matchedPhotoId) {
      continue
    }

    if (!VALID_LOCATION_ID_SET.has(matchedPhotoId)) {
      console.warn('[AI] Keyword override candidate rejected (invalid id)', {
        source: source.content.slice(0, 120),
        candidate: matchedPhotoId
      })
      continue
    }

    if (originalCall?.arguments.photoId === matchedPhotoId) {
      console.info('[AI] Keyword override matched existing destination', {
        source: source.content.slice(0, 120),
        photoId: matchedPhotoId
      })
      return {
        call: originalCall,
        overridden: false
      }
    }

    // Do NOT synthesize a navigate_to call without explicit model intent.
    // Only adjust the destination if the model already requested navigation.
    if (originalCall && originalCall.name === 'navigate_to') {
      console.info('[AI] Keyword override adjusted destination for existing navigation call', {
        source: source.content.slice(0, 120),
        from: originalCall.arguments.photoId,
        to: matchedPhotoId
      })
      return {
        call: {
          name: 'navigate_to',
          arguments: { photoId: matchedPhotoId }
        },
        overridden: true
      }
    }

    // PROACTIVE OVERRIDE: If AI explicitly failed + we have strong keyword match, provide fallback
    const isVectorStoreFailure = 
      assistantMessage?.includes("don't have any information regarding that specific location") ||
      assistantMessage?.includes("don't have information about that specific location")

    if (isVectorStoreFailure && source.role === 'user') {
      const fallbackMessage = generateFallbackLocationResponse(matchedPhotoId, currentLocation)
      
      // Only provide fallback if location was successfully validated AND is routable
      if (fallbackMessage) {
        console.warn('[AI] Vector store search failed but keyword match found and validated; providing proactive fallback', {
          userQuery: source.content.slice(0, 120),
          matchedPhotoId,
          currentLocation,
          source: 'keyword-override-validated'
        })

        return {
          call: null,
          overridden: true,
          fallbackMessage
        }
      } else {
        console.warn('[AI] Keyword match found but location validation/pathfinding failed; cannot provide fallback', {
          userQuery: source.content.slice(0, 120),
          matchedPhotoId,
          currentLocation
        })
      }
    }

    // No existing tool call and no failure: respect confirmation workflow and do not create one.
    console.info('[AI] Keyword match found but no tool call present; awaiting user confirmation')
    return {
      call: null,
      overridden: false
    }
  }

  if (originalCall) {
    console.info('[AI] No keyword override applied', {
      sourceCount: sources.length,
      destination: originalCall.arguments.photoId
    })
  } else {
    console.info('[AI] No keyword override applied', {
      sourceCount: sources.length,
      destination: null
    })
  }

  return {
    call: originalCall,
    overridden: false
  }
}

function augmentFunctionCallWithPath(
  originalCall: FunctionCall | null,
  currentLocation: string
): FunctionCall | null {
  if (!originalCall) {
    return null
  }

  // Pass through non-navigation function calls unchanged
  if (originalCall.name !== 'navigate_to') {
    return originalCall
  }

  if (originalCall.arguments.error) {
    return originalCall
  }

  if (!currentLocation) {
    return {
      name: 'navigate_to',
      arguments: {
        photoId: originalCall.arguments.photoId,
        error: 'Current location is unavailable for pathfinding.'
      }
    }
  }

  const destinationId = originalCall.arguments.photoId
  const resolvedDestinationId = resolvePhotoId(destinationId)
  const pathResult = findPath(currentLocation, resolvedDestinationId)

  if (!pathResult) {
    console.warn('[AI] Pathfinding failed', {
      from: currentLocation,
      to: destinationId
    })
    return {
      name: 'navigate_to',
      arguments: {
        photoId: resolvedDestinationId,
        error: 'Unable to calculate a route from the current location to the requested destination.'
      }
    }
  }

  if (!validatePath(pathResult)) {
    console.warn('[AI] Path validation failed', {
      from: currentLocation,
      to: destinationId,
      path: pathResult.path
    })
    return {
      name: 'navigate_to',
      arguments: {
        photoId: resolvedDestinationId,
        error: 'Calculated route failed validation. Please try again.'
      }
    }
  }

  const routeDescription = getRouteDescription(pathResult)
  const finalOrientation = getFinalOrientation(destinationId)

  console.info('[AI] Pathfinding succeeded', {
    from: currentLocation,
    to: destinationId,
    steps: pathResult.distance,
    path: pathResult.path,
    finalOrientation
  })

  return {
    name: 'navigate_to',
    arguments: {
      photoId: resolvedDestinationId,
      path: pathResult.path,
      distance: pathResult.distance,
      routeDescription,
      finalOrientation
    }
  }
}

function getTotalCharacterCount(messages: ChatMessage[]): number {
  return messages.reduce((total, message) => total + message.content.length, 0)
}

function validateMessages(messages: ChatMessage[]): string | null {
  if (!messages || messages.length === 0) {
    return 'No messages provided.'
  }
  if (messages.length > MAX_MESSAGE_COUNT) {
    return 'Conversation too long. Please start a new chat.'
  }
  const totalCharacters = getTotalCharacterCount(messages)
  if (totalCharacters > MAX_TOTAL_CHARACTERS) {
    return 'Message too long. Please be more concise.'
  }
  return null
}

function shouldSummariseConversation(messageCount: number, totalCharacters: number): boolean {
  return messageCount >= SUMMARY_TRIGGER_THRESHOLD || totalCharacters >= SUMMARY_CHARACTER_THRESHOLD
}

function buildMessagesForRequest(summary: string | null, messages: ChatMessage[]): ChatMessage[] {
  const trimmedSummary = summary?.trim()
  if (trimmedSummary) {
    return [
      {
        role: 'system',
        content: `Conversation summary: ${trimmedSummary}`
      },
      ...messages
    ]
  }
  return messages
}

type SummariseConversationInput = {
  priorSummary: string | null
  messages: ChatMessage[]
}

type SummariseConversationResult = {
  summary: string | null
  updated: boolean
}

async function summariseConversation({
  priorSummary,
  messages
}: SummariseConversationInput): Promise<SummariseConversationResult> {
  if (!messages.length) {
    return {
      summary: priorSummary ?? null,
      updated: false
    }
  }

  try {
    const client = getOpenAIClient()
    const rawResponse = await client.responses.create({
      model: 'gpt-4o-mini',
      input: [
        {
          role: 'system',
          content: SUMMARISATION_SYSTEM_PROMPT,
          type: 'message'
        },
        ...(priorSummary
          ? [
              {
                role: 'system' as const,
                content: `Previous summary:\n${priorSummary}`,
                type: 'message' as const
              }
            ]
          : []),
        ...normaliseMessageInput(messages)
      ],
      temperature: 0.2,
      max_output_tokens: 200
    } satisfies ResponseCreateParamsNonStreaming)
    const response = ensureNonStreamingResponse(rawResponse)

    const summary = response.output_text?.trim() ?? parseResponseText(response.output) ?? null
    if (summary && summary.length > 0) {
      console.info('[AI] Conversation summary generated', {
        hadPriorSummary: !!priorSummary,
        summarisedMessageCount: messages.length,
        summaryLength: summary.length
      })
      return {
        summary,
        updated: summary !== (priorSummary ?? null)
      }
    }
  } catch (error) {
    console.error('[AI] Failed to summarise conversation', error)
  }
  return {
    summary: priorSummary ?? null,
    updated: false
  }
}

function handleKnownApiErrors(error: unknown): ChatResponse | null {
  if (error instanceof APIError) {
    if (error.status === 429) {
      return {
        message: null,
        functionCall: null,
        error: 'The AI is responding to many requests right now. Please try again shortly.'
      }
    }
    if (error.status === 401) {
      return {
        message: null,
        functionCall: null,
        error: 'AI service configuration error. Check your API credentials.'
      }
    }
    if (typeof error.status === 'number' && error.status >= 500) {
      return {
        message: null,
        functionCall: null,
        error: 'The AI service is temporarily unavailable. Please try again.'
      }
    }
  }
  if (
    typeof error === 'object' &&
    error !== null &&
    'code' in error &&
    (error as { code?: string }).code &&
    ['ECONNREFUSED', 'ETIMEDOUT'].includes((error as { code: string }).code)
  ) {
    return {
      message: null,
      functionCall: null,
      error: 'Network error while contacting the AI service. Please check your connection.'
    }
  }
  return null
}

interface ExecuteChatInput {
  messages: ChatMessage[]
  currentLocation: string
}

/**
 * Generates an OpenAI navigation response using the supplied chat history
 *
 * Validates the message payload, assembles the navigation system prompt, and
 * invokes the Responses API so the assistant can reply with directions or
 * trigger the navigation tool call.
 *
 * @param input - Request details for the current turn
 * @param input.messages - Chat history supplied to the model in chronological order
 * @param input.currentLocation - Viewer photo identifier describing the user’s current position
 * @returns ChatResponse containing an assistant message, optional tool call, or an error description
 *
 * @example
 * ```typescript
 * const response = await executeChat({
 *   messages: [{ role: 'user', content: 'Where is the student lounge?' }],
 *   currentLocation: 'a-f1-north-entrance'
 * })
 * ```
 */
export async function executeChat({ messages, currentLocation }: ExecuteChatInput): Promise<ChatResponse> {
  console.info('[AI] getChatResponse invoked', {
    messageCount: messages?.length ?? 0,
    currentLocation
  })

  if (!currentLocation) {
    return {
      message: null,
      functionCall: null,
      error: 'Current location is required for navigation.'
    }
  }

  const validationError = validateMessages(messages)
  if (validationError) {
    return {
      message: null,
      functionCall: null,
      error: validationError
    }
  }

  try {
    const client = getOpenAIClient()
    const vectorStoreId = getVectorStoreId()

    console.info('[AI] Preparing OpenAI request', {
      vectorStoreId,
      messageCount: messages.length
    })

    const rawResponse = await client.responses.create({
      model: 'gpt-4o-mini',
      input: [
        {
          role: 'system',
          content: buildSystemPrompt(currentLocation),
          type: 'message'
        },
        ...normaliseMessageInput(messages)
      ],
      tools: [
        NAVIGATION_TOOL,
        DOCUMENTS_TOOL,
        AVAILABLE_LOCATIONS_TOOL,
        {
          type: 'file_search' as const,
          vector_store_ids: [vectorStoreId]
        }
      ],
      temperature: 0.0,
      max_output_tokens: 250
    } satisfies ResponseCreateParamsNonStreaming)
    const response = ensureNonStreamingResponse(rawResponse)

    const message = sanitiseAssistantMessage(
      response.output_text?.trim() ?? parseResponseText(response.output) ?? null
    )
    const functionCall = parseFunctionCall(response.output)

    const { call: intentAwareFunctionCall, overridden, fallbackMessage } = applyKeywordOverrides(
      functionCall,
      message,
      messages,
      currentLocation
    )
    let pathAwareFunctionCall = augmentFunctionCallWithPath(intentAwareFunctionCall, currentLocation)

    if (!pathAwareFunctionCall && overridden && functionCall) {
      console.warn('[AI] Override path failed; falling back to original destination', {
        overrideDestination: intentAwareFunctionCall?.arguments.photoId,
        fallbackDestination: functionCall.arguments.photoId
      })
      pathAwareFunctionCall = augmentFunctionCallWithPath(functionCall, currentLocation)
    }

    // Use fallback message if provided (when vector store fails but keyword matching succeeds)
    const finalMessage = fallbackMessage || message

    console.info('[AI] OpenAI response summary', {
      hasMessage: !!finalMessage,
      hasFunctionCall: !!pathAwareFunctionCall,
      usedFallback: !!fallbackMessage,
      pathStepCount: pathAwareFunctionCall?.arguments.path?.length ?? 0,
      pathError: pathAwareFunctionCall?.arguments.error ?? null
    })

    return {
      message: finalMessage,
      functionCall: pathAwareFunctionCall
    }
  } catch (error) {
    console.error('[AI] Response generation failed', error)
    const handled = handleKnownApiErrors(error)
    if (handled) {
      return handled
    }
    return {
      message: null,
      functionCall: null,
      error: 'Sorry, something went wrong while contacting the AI service.'
    }
  }
}

/**
 * Generates an AI response while maintaining a rolling conversation summary
 *
 * Automatically compacts older chat turns into a structured summary when the
 * history approaches model limits, ensuring users can continue longer sessions
 * without encountering hard reset errors.
 *
 * @param input - Payload containing the previous state and the next message
 * @param input.state - Previously persisted conversation memory or `null` for new chats
 * @param input.nextMessage - Latest chat message to process
 * @param input.currentLocation - Viewer photo identifier representing the user’s position
 * @returns ExecuteChatWithSummariesResult containing the assistant reply and updated memory state
 *
 * @example
 * ```typescript
 * const result = await executeChatWithSummaries({
 *   state: { summary: null, messages: [] },
 *   nextMessage: { role: 'user', content: 'Where is the library?' },
 *   currentLocation: 'a-f1-north-entrance'
 * })
 * console.log(result.response.message)
 * ```
 */
export async function executeChatWithSummaries({
  state,
  nextMessage,
  currentLocation
}: ExecuteChatWithSummariesInput): Promise<ExecuteChatWithSummariesResult> {
  const baseState: ConversationState = {
    summary: state?.summary ?? null,
    messages: Array.isArray(state?.messages) ? state.messages : []
  }

  const normalisedNextMessage: ChatMessage = {
    role: nextMessage.role,
    content: nextMessage.content.trim()
  }

  if (!normalisedNextMessage.content) {
    return {
      response: {
        message: null,
        functionCall: null,
        error: 'Message cannot be empty.'
      },
      state: baseState
    }
  }

  const combinedMessages = [...baseState.messages, normalisedNextMessage]
  const totalCharacters = getTotalCharacterCount(combinedMessages)

  let workingSummary = baseState.summary
  let workingMessages = combinedMessages

  // Check if we're in a confirmation state - if last assistant message asks for navigation
  const lastAssistantMessage = [...baseState.messages].reverse().find(message => message.role === 'assistant')
  const isPendingNavigationConfirmation = isNavigationOffer(lastAssistantMessage?.content)
  const isPendingDocumentsConfirmation = isDocumentsOffer(lastAssistantMessage?.content)
  const isPendingConfirmation = isPendingNavigationConfirmation || isPendingDocumentsConfirmation
  const pendingConfirmationPhotoId = isPendingNavigationConfirmation
    ? extractPendingDestinationId(lastAssistantMessage, baseState.messages)
    : null
  const userConfirmedNavigation = isPendingNavigationConfirmation && isAffirmativeUserResponse(normalisedNextMessage.content)
  const userConfirmedDocuments = isPendingDocumentsConfirmation && isAffirmativeUserResponse(normalisedNextMessage.content)

  const needsSummarisation =
    !isPendingConfirmation &&
    shouldSummariseConversation(combinedMessages.length, totalCharacters) &&
    combinedMessages.length > SUMMARY_TAIL_MESSAGE_COUNT

  if (isPendingConfirmation && shouldSummariseConversation(combinedMessages.length, totalCharacters)) {
    console.info('[AI] Summarization skipped - awaiting navigation confirmation', {
      messageCount: combinedMessages.length,
      lastAssistantMessagePreview: lastAssistantMessage?.content?.slice(-80)
    })
  }

  if (needsSummarisation) {
    const sliceIndex = Math.max(combinedMessages.length - SUMMARY_TAIL_MESSAGE_COUNT, 0)
    const messagesToSummarise = combinedMessages.slice(0, sliceIndex)
    const messagesToKeep = combinedMessages.slice(sliceIndex)

    if (messagesToSummarise.length > 0) {
      const summaryResult = await summariseConversation({
        priorSummary: baseState.summary,
        messages: messagesToSummarise
      })

      if (summaryResult.summary) {
        workingSummary = summaryResult.summary
      }

      if (summaryResult.updated && summaryResult.summary) {
        workingMessages = messagesToKeep
        console.info('[AI] Rolled conversation into summary', {
          retainedMessages: workingMessages.length,
          summarisedCount: messagesToSummarise.length
        })
      } else if (combinedMessages.length > MAX_MESSAGE_COUNT) {
        workingMessages = combinedMessages.slice(-MAX_MESSAGE_COUNT)
        console.warn('[AI] Summary unchanged; trimming conversation to last turns', {
          retainedMessages: workingMessages.length
        })
      }
    }
  } else if (combinedMessages.length > MAX_MESSAGE_COUNT) {
    workingMessages = combinedMessages.slice(-MAX_MESSAGE_COUNT)
  }

  let messagesForRequest = buildMessagesForRequest(workingSummary, workingMessages)

  if (messagesForRequest.length > MAX_MESSAGE_COUNT) {
    const systemMessages = messagesForRequest.filter(message => message.role === 'system')
    const nonSystemMessages = messagesForRequest.filter(message => message.role !== 'system')
    const keptSystemMessages = systemMessages.slice(-1)
    const allowedNonSystemCount = Math.max(MAX_MESSAGE_COUNT - keptSystemMessages.length, 0)
    messagesForRequest = [
      ...keptSystemMessages,
      ...nonSystemMessages.slice(-allowedNonSystemCount)
    ]
  }

  let response = await executeChat({
    messages: messagesForRequest,
    currentLocation
  })

  if (userConfirmedNavigation && !response.functionCall && pendingConfirmationPhotoId) {
    console.warn('[AI] Navigation confirmation detected but tool call missing. Synthesizing navigation command.', {
      destination: pendingConfirmationPhotoId
    })
    const fallbackCall = augmentFunctionCallWithPath(
      {
        name: 'navigate_to',
        arguments: { photoId: pendingConfirmationPhotoId }
      },
      currentLocation
    )
    if (fallbackCall) {
      response = {
        message: null,
        functionCall: fallbackCall
      }
    } else {
      console.error('[AI] Failed to synthesise navigation command after confirmation', {
        destination: pendingConfirmationPhotoId
      })
    }
  }

  if (userConfirmedDocuments && !response.functionCall) {
    console.warn('[AI] Documents confirmation detected but tool call missing. Synthesizing documents command.')
    response = {
      message: null,
      functionCall: {
        name: 'show_campus_documents',
        arguments: {}
      }
    }
  }

  const userRequestedAvailableLocations = isAvailableLocationsRequest(normalisedNextMessage.content)
  if (
    userRequestedAvailableLocations &&
    (!response.functionCall || response.functionCall.name !== 'show_available_locations')
  ) {
    console.info('[AI] Synthesizing available locations command based on user request.', {
      userMessage: normalisedNextMessage.content
    })
    response = {
      message: null,
      functionCall: {
        name: 'show_available_locations',
        arguments: {}
      }
    }
  }

  const sanitisedMessage = sanitiseAssistantMessage(response.message)
  const routeDescriptionFallback = response.functionCall?.name === 'navigate_to' 
    ? response.functionCall.arguments.routeDescription
    : null
  const defaultFunctionNotice = response.functionCall
    ? response.functionCall.name === 'navigate_to'
      ? `Navigation command issued for ${response.functionCall.arguments.photoId}.`
      : 'Opening campus documents.'
    : null
  const assistantContent = sanitiseAssistantMessage(
    sanitisedMessage ?? routeDescriptionFallback ?? defaultFunctionNotice
  )

  let nextStateMessages = workingMessages

  if (assistantContent) {
    nextStateMessages = [
      ...nextStateMessages,
      {
        role: 'assistant',
        content: assistantContent.trim()
      }
    ]
  }

  if (nextStateMessages.length > MAX_MESSAGE_COUNT) {
    nextStateMessages = nextStateMessages.slice(-MAX_MESSAGE_COUNT)
  }

  return {
    response: {
      ...response,
      message: sanitisedMessage
    },
    state: {
      summary: workingSummary,
      messages: nextStateMessages
    }
  }
}
