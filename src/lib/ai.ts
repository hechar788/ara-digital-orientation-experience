'use server'

import OpenAI, { APIError } from 'openai'
import type { Response as OpenAIResponse, ResponseCreateParamsNonStreaming } from 'openai/resources/responses/responses'
import { LOCATION_KEYWORD_OVERRIDES, VALID_LOCATION_ID_SET } from './ai.locations'
import { buildSystemPrompt, NAVIGATION_TOOL, SUMMARISATION_SYSTEM_PROMPT } from './ai.prompts'
import { findPath, getRouteDescription, validatePath } from './pathfinding'
let cachedClient: OpenAI | null = null
type GeneratedResponse = OpenAIResponse
type CreateResponseResult = Awaited<ReturnType<OpenAI['responses']['create']>>

function getOpenAIClient(): OpenAI {
  if (!cachedClient) {
    const apiKey = process.env.OPENAI_API_KEY

    if (!apiKey) {
      throw new Error('OPENAI_API_KEY is not set. Add it to your environment before using getChatResponse().')
    }

    cachedClient = new OpenAI({
      apiKey
    })
  }

  return cachedClient
}

function getVectorStoreId(): string {
  const vectorStoreId = process.env.OPENAI_LOCATIONS_VECTOR_STORE_ID

  if (!vectorStoreId) {
    throw new Error('OPENAI_LOCATIONS_VECTOR_STORE_ID is not set. Add it to your environment before using getChatResponse().')
  }

  return vectorStoreId
}

function isNonStreamingResponse(response: CreateResponseResult): response is GeneratedResponse {
  return typeof response === 'object' && response !== null && 'output' in response
}

function ensureNonStreamingResponse(response: CreateResponseResult): GeneratedResponse {
  if (isNonStreamingResponse(response)) {
    return response
  }
  throw new Error('OpenAI returned a streaming response; expected non-streaming output.')
}

/**
 * Describes a chat message exchanged with the AI assistant
 *
 * Used to pass prior conversation history back to the Responses API so the model
 * can reply with awareness of previous turns.
 *
 * @property role - Sender role for the message (`user`, `assistant`, or `system`)
 * @property content - Message body text provided to the model
 *
 * @example
 * ```typescript
 * const message: ChatMessage = { role: 'user', content: 'Where is the cafe?' }
 * ```
 */
export interface ChatMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

/**
 * Represents a navigation function call generated by the AI assistant
 *
 * Extended in Phase 4 to bundle BFS pathfinding metadata so the client can
 * present step-by-step routes rather than teleporting directly to the
 * destination. Errors produced during path calculation are surfaced via the
 * optional `error` field so the UI can display friendly fallback messaging.
 *
 * @property name - Function identifier returned by the model (always `navigate_to`)
 * @property arguments - Navigation payload supplied with the function call
 * @property arguments.photoId - Destination campus photo identifier provided by the AI
 * @property arguments.path - Optional ordered list of photo IDs describing the computed route
 * @property arguments.distance - Optional number of steps contained in the route
 * @property arguments.routeDescription - Optional human-readable summary of the route
 * @property arguments.error - Optional explanation when path calculation fails
 *
 * @example
 * ```typescript
 * const functionCall: FunctionCall = {
 *   name: 'navigate_to',
 *   arguments: {
 *     photoId: 'library-f1-entrance',
 *     path: ['a-f1-north-entrance', 'a-f1-hallway', 'library-f1-entrance'],
 *     distance: 2,
 *     routeDescription: 'Route found: 2 steps from A Block F1 (North Entrance) to Library F1 Entrance.'
 *   }
 * }
 * ```
 */
export interface FunctionCall {
  name: 'navigate_to'
  arguments: {
    photoId: string
    path?: string[]
    distance?: number
    routeDescription?: string
    error?: string
  }
}

/**
 * Structured response returned from getChatResponse
 *
 * Used by the UI to differentiate between plain text replies, navigation tool
 * invocations, and recoverable errors.
 *
 * @property message - Assistant text reply or `null` when an error occurs
 * @property functionCall - Navigation command selected by the AI assistant
 * @property error - Present when the request fails for any reason
 *
 * @example
 * ```typescript
 * const response: ChatResponse = {
 *   message: 'Head through the atrium, then turn left toward the library entrance.',
 *   functionCall: {
 *     name: 'navigate_to',
 *     arguments: {
 *       photoId: 'library-f1-entrance',
 *       path: ['a-f1-north-entrance', 'a-f1-hallway', 'library-f1-entrance'],
 *       distance: 2,
 *       routeDescription: 'Route found: 2 steps from A Block F1 (North Entrance) to Library F1 Entrance.'
 *     }
 *   }
 * }
 * ```
 */
export interface ChatResponse {
  message: string | null
  functionCall: FunctionCall | null
  error?: string
}

/**
 * Maintains condensed chat history that survives across requests
 *
 * Tracks the rolling summary generated by the AI alongside the most recent
 * granular exchanges so future turns can reuse prior context without sending
 * the full transcript.
 *
 * @property summary - Condensed narrative of the conversation so far or `null` when no summary exists
 * @property messages - Recent verbatim chat messages preserved for precise follow-up handling
 *
 * @example
 * ```typescript
 * const state: ConversationState = {
 *   summary: 'Goal: Locate the library. Confirmed: navigation requested to library-f1-entrance.',
 *   messages: [{ role: 'user', content: 'Can you take me to the library?' }]
 * }
 * ```
 */
export interface ConversationState {
  summary: string | null
  messages: ChatMessage[]
}

/**
 * Payload accepted by executeChatWithSummaries
 *
 * Carries the conversation memory from the client together with the next user
 * message and the viewer's current location identifier.
 *
 * @property state - Previously returned conversation state or `null` when the chat is new
 * @property nextMessage - Next chat turn to process, typically the latest user input
 * @property currentLocation - Viewer photo identifier describing the user's position
 *
 * @example
 * ```typescript
 * const input: ExecuteChatWithSummariesInput = {
 *   state: { summary: null, messages: [] },
 *   nextMessage: { role: 'user', content: 'Where is the cafeteria?' },
 *   currentLocation: 'a-f1-north-entrance'
 * }
 * ```
 */
export interface ExecuteChatWithSummariesInput {
  state: ConversationState | null
  nextMessage: ChatMessage
  currentLocation: string
}

/**
 * Result structure returned by executeChatWithSummaries
 *
 * Bundles the assistant response with the updated conversation memory that the
 * client should persist for the next request.
 *
 * @property response - Chat output including optional navigation tool call
 * @property state - Conversation state reflecting any new summaries and trimmed history
 *
 * @example
 * ```typescript
 * const result = await executeChatWithSummaries({
 *   state: { summary: null, messages: [] },
 *   nextMessage: { role: 'user', content: 'Take me to the gym' },
 *   currentLocation: 'a-f1-north-entrance'
 * })
 * console.log(result.state.summary)
 * // "Goal: Reach the gym. Confirmed navigation requested."
 * ```
 */
export interface ExecuteChatWithSummariesResult {
  response: ChatResponse
  state: ConversationState
}

const MAX_MESSAGE_COUNT = 20
const MAX_TOTAL_CHARACTERS = 5000
const SUMMARY_TRIGGER_THRESHOLD = 10
const SUMMARY_CHARACTER_THRESHOLD = 3500
const SUMMARY_TAIL_MESSAGE_COUNT = 6

function parseResponseText(output: GeneratedResponse['output']): string | null {
  const parts: string[] = []
  for (const item of output ?? []) {
    if (item.type === 'message' && Array.isArray(item.content)) {
      for (const contentPart of item.content) {
        if (contentPart.type === 'output_text' && contentPart.text) {
          parts.push(contentPart.text)
        }
      }
    }
  }
  const combined = parts.join('').trim()
  return combined.length > 0 ? combined : null
}

function parseFunctionCall(output: GeneratedResponse['output']): FunctionCall | null {
  for (const item of output ?? []) {
    if (item.type === 'function_call' && item.name === 'navigate_to') {
      try {
        const parsedArguments = JSON.parse(item.arguments ?? '{}')
        const photoId = typeof parsedArguments.photoId === 'string' ? parsedArguments.photoId : undefined
        if (photoId && VALID_LOCATION_ID_SET.has(photoId)) {
          return {
            name: 'navigate_to',
            arguments: { photoId }
          }
        }
      } catch (error) {
        console.error('[AI] Failed to parse function call arguments', error)
      }
    }
  }
  return null
}

function normaliseMessageInput(messages: ChatMessage[]) {
  return messages.map(message => ({
    role: message.role,
    content: message.content,
    type: 'message' as const
  }))
}

function sanitiseAssistantMessage(text: string | null): string | null {
  if (!text) {
    return null
  }
  const normalised = text.replace(/\r\n/g, '\n')
  const collapsedQuotes = normalised
    .replace(/"([^"\n]*?)\s*\n+\s*"/g, (_match, content: string) => `"${content.trim()}"`)
    .replace(/\s*\n{3,}\s*/g, '\n\n')
    .trim()
  const joinedConjunctions = collapsedQuotes.replace(/\b(or)\s*\n+(?=[a-z])/gi, (_, conj: string) => `${conj} `)
  const normalisedCase = joinedConjunctions.replace(/\bor\s+([A-Z])/g, (_match, letter: string) => `or ${letter.toLowerCase()}`)
  return normalisedCase.replace(/\*\*(.*?)\*\*/g, (_match, content: string) => content)
}

function findOverrideFromText(text: string | null): string | null {
  if (!text) {
    return null
  }
  const normalised = text.toLowerCase()
  for (const mapping of LOCATION_KEYWORD_OVERRIDES) {
    if (mapping.keywords.some(keyword => normalised.includes(keyword))) {
      return mapping.photoId
    }
  }
  return null
}

function findLatestUserMessage(messages: ChatMessage[]): string | null {
  for (let index = messages.length - 1; index >= 0; index -= 1) {
    const message = messages[index]
    if (message.role === 'user') {
      return message.content
    }
  }
  return null
}

function applyKeywordOverrides(
  originalCall: FunctionCall | null,
  assistantMessage: string | null,
  messages: ChatMessage[]
): FunctionCall | null {
  if (!originalCall) {
    return null
  }
  const sources: Array<string | null> = [assistantMessage, findLatestUserMessage(messages)]
  for (const source of sources) {
    const overridePhotoId = findOverrideFromText(source)
    if (overridePhotoId && overridePhotoId !== originalCall.arguments.photoId && VALID_LOCATION_ID_SET.has(overridePhotoId)) {
      return {
        name: 'navigate_to',
        arguments: {
          photoId: overridePhotoId
        }
      }
    }
  }
  return originalCall
}

function augmentFunctionCallWithPath(
  originalCall: FunctionCall | null,
  currentLocation: string
): FunctionCall | null {
  if (!originalCall) {
    return null
  }

  if (originalCall.arguments.error) {
    return originalCall
  }

  if (!currentLocation) {
    return {
      name: originalCall.name,
      arguments: {
        photoId: originalCall.arguments.photoId,
        error: 'Current location is unavailable for pathfinding.'
      }
    }
  }

  const destinationId = originalCall.arguments.photoId
  const pathResult = findPath(currentLocation, destinationId)

  if (!pathResult) {
    return {
      name: originalCall.name,
      arguments: {
        photoId: destinationId,
        error: 'Unable to calculate a route from the current location to the requested destination.'
      }
    }
  }

  if (!validatePath(pathResult)) {
    return {
      name: originalCall.name,
      arguments: {
        photoId: destinationId,
        error: 'Calculated route failed validation. Please try again.'
      }
    }
  }

  const routeDescription = getRouteDescription(pathResult)

  return {
    name: originalCall.name,
    arguments: {
      photoId: destinationId,
      path: pathResult.path,
      distance: pathResult.distance,
      routeDescription
    }
  }
}

function getTotalCharacterCount(messages: ChatMessage[]): number {
  return messages.reduce((total, message) => total + message.content.length, 0)
}

function validateMessages(messages: ChatMessage[]): string | null {
  if (!messages || messages.length === 0) {
    return 'No messages provided.'
  }
  if (messages.length > MAX_MESSAGE_COUNT) {
    return 'Conversation too long. Please start a new chat.'
  }
  const totalCharacters = getTotalCharacterCount(messages)
  if (totalCharacters > MAX_TOTAL_CHARACTERS) {
    return 'Message too long. Please be more concise.'
  }
  return null
}

function shouldSummariseConversation(messageCount: number, totalCharacters: number): boolean {
  return messageCount >= SUMMARY_TRIGGER_THRESHOLD || totalCharacters >= SUMMARY_CHARACTER_THRESHOLD
}

function buildMessagesForRequest(summary: string | null, messages: ChatMessage[]): ChatMessage[] {
  const trimmedSummary = summary?.trim()
  if (trimmedSummary) {
    return [
      {
        role: 'system',
        content: `Conversation summary: ${trimmedSummary}`
      },
      ...messages
    ]
  }
  return messages
}

type SummariseConversationInput = {
  priorSummary: string | null
  messages: ChatMessage[]
}

type SummariseConversationResult = {
  summary: string | null
  updated: boolean
}

async function summariseConversation({
  priorSummary,
  messages
}: SummariseConversationInput): Promise<SummariseConversationResult> {
  if (!messages.length) {
    return {
      summary: priorSummary ?? null,
      updated: false
    }
  }

  try {
    const client = getOpenAIClient()
    const rawResponse = await client.responses.create({
      model: 'gpt-4o-mini',
      input: [
        {
          role: 'system',
          content: SUMMARISATION_SYSTEM_PROMPT,
          type: 'message'
        },
        ...(priorSummary
          ? [
              {
                role: 'system' as const,
                content: `Previous summary:\n${priorSummary}`,
                type: 'message' as const
              }
            ]
          : []),
        ...normaliseMessageInput(messages)
      ],
      temperature: 0.2,
      max_output_tokens: 200
    } satisfies ResponseCreateParamsNonStreaming)
    const response = ensureNonStreamingResponse(rawResponse)

    const summary = response.output_text?.trim() ?? parseResponseText(response.output) ?? null
    if (summary && summary.length > 0) {
      console.info('[AI] Conversation summary generated', {
        hadPriorSummary: !!priorSummary,
        summarisedMessageCount: messages.length,
        summaryLength: summary.length
      })
      return {
        summary,
        updated: summary !== (priorSummary ?? null)
      }
    }
  } catch (error) {
    console.error('[AI] Failed to summarise conversation', error)
  }
  return {
    summary: priorSummary ?? null,
    updated: false
  }
}

function handleKnownApiErrors(error: unknown): ChatResponse | null {
  if (error instanceof APIError) {
    if (error.status === 429) {
      return {
        message: null,
        functionCall: null,
        error: 'The AI is responding to many requests right now. Please try again shortly.'
      }
    }
    if (error.status === 401) {
      return {
        message: null,
        functionCall: null,
        error: 'AI service configuration error. Check your API credentials.'
      }
    }
    if (typeof error.status === 'number' && error.status >= 500) {
      return {
        message: null,
        functionCall: null,
        error: 'The AI service is temporarily unavailable. Please try again.'
      }
    }
  }
  if (
    typeof error === 'object' &&
    error !== null &&
    'code' in error &&
    (error as { code?: string }).code &&
    ['ECONNREFUSED', 'ETIMEDOUT'].includes((error as { code: string }).code)
  ) {
    return {
      message: null,
      functionCall: null,
      error: 'Network error while contacting the AI service. Please check your connection.'
    }
  }
  return null
}

interface ExecuteChatInput {
  messages: ChatMessage[]
  currentLocation: string
}

/**
 * Generates an OpenAI navigation response using the supplied chat history
 *
 * Validates the message payload, assembles the navigation system prompt, and
 * invokes the Responses API so the assistant can reply with directions or
 * trigger the navigation tool call.
 *
 * @param input - Request details for the current turn
 * @param input.messages - Chat history supplied to the model in chronological order
 * @param input.currentLocation - Viewer photo identifier describing the user’s current position
 * @returns ChatResponse containing an assistant message, optional tool call, or an error description
 *
 * @example
 * ```typescript
 * const response = await executeChat({
 *   messages: [{ role: 'user', content: 'Where is the student lounge?' }],
 *   currentLocation: 'a-f1-north-entrance'
 * })
 * ```
 */
export async function executeChat({ messages, currentLocation }: ExecuteChatInput): Promise<ChatResponse> {
  console.info('[AI] getChatResponse invoked', {
    messageCount: messages?.length ?? 0,
    currentLocation
  })

  if (!currentLocation) {
    return {
      message: null,
      functionCall: null,
      error: 'Current location is required for navigation.'
    }
  }

  const validationError = validateMessages(messages)
  if (validationError) {
    return {
      message: null,
      functionCall: null,
      error: validationError
    }
  }

  try {
    const client = getOpenAIClient()
    const vectorStoreId = getVectorStoreId()

    console.info('[AI] Preparing OpenAI request', {
      vectorStoreId,
      messageCount: messages.length
    })

    const rawResponse = await client.responses.create({
      model: 'gpt-4o-mini',
      input: [
        {
          role: 'system',
          content: buildSystemPrompt(currentLocation),
          type: 'message'
        },
        ...normaliseMessageInput(messages)
      ],
      tools: [
        {
          type: 'file_search',
          vector_store_ids: [vectorStoreId]
        },
        NAVIGATION_TOOL
      ],
      temperature: 0.2,
      max_output_tokens: 200
    } satisfies ResponseCreateParamsNonStreaming)
    const response = ensureNonStreamingResponse(rawResponse)

    const message = sanitiseAssistantMessage(
      response.output_text?.trim() ?? parseResponseText(response.output) ?? null
    )
    const functionCall = parseFunctionCall(response.output)

    const adjustedFunctionCall = applyKeywordOverrides(functionCall, message, messages)
    const pathAwareFunctionCall = augmentFunctionCallWithPath(adjustedFunctionCall, currentLocation)

    console.info('[AI] OpenAI response summary', {
      hasMessage: !!message,
      hasFunctionCall: !!pathAwareFunctionCall,
      pathStepCount: pathAwareFunctionCall?.arguments.path?.length ?? 0,
      pathError: pathAwareFunctionCall?.arguments.error ?? null
    })

    return {
      message,
      functionCall: pathAwareFunctionCall
    }
  } catch (error) {
    console.error('[AI] Response generation failed', error)
    const handled = handleKnownApiErrors(error)
    if (handled) {
      return handled
    }
    return {
      message: null,
      functionCall: null,
      error: 'Sorry, something went wrong while contacting the AI service.'
    }
  }
}

/**
 * Generates an AI response while maintaining a rolling conversation summary
 *
 * Automatically compacts older chat turns into a structured summary when the
 * history approaches model limits, ensuring users can continue longer sessions
 * without encountering hard reset errors.
 *
 * @param input - Payload containing the previous state and the next message
 * @param input.state - Previously persisted conversation memory or `null` for new chats
 * @param input.nextMessage - Latest chat message to process
 * @param input.currentLocation - Viewer photo identifier representing the user’s position
 * @returns ExecuteChatWithSummariesResult containing the assistant reply and updated memory state
 *
 * @example
 * ```typescript
 * const result = await executeChatWithSummaries({
 *   state: { summary: null, messages: [] },
 *   nextMessage: { role: 'user', content: 'Where is the library?' },
 *   currentLocation: 'a-f1-north-entrance'
 * })
 * console.log(result.response.message)
 * ```
 */
export async function executeChatWithSummaries({
  state,
  nextMessage,
  currentLocation
}: ExecuteChatWithSummariesInput): Promise<ExecuteChatWithSummariesResult> {
  const baseState: ConversationState = {
    summary: state?.summary ?? null,
    messages: Array.isArray(state?.messages) ? state.messages : []
  }

  const normalisedNextMessage: ChatMessage = {
    role: nextMessage.role,
    content: nextMessage.content.trim()
  }

  if (!normalisedNextMessage.content) {
    return {
      response: {
        message: null,
        functionCall: null,
        error: 'Message cannot be empty.'
      },
      state: baseState
    }
  }

  const combinedMessages = [...baseState.messages, normalisedNextMessage]
  const totalCharacters = getTotalCharacterCount(combinedMessages)

  let workingSummary = baseState.summary
  let workingMessages = combinedMessages

  const needsSummarisation =
    shouldSummariseConversation(combinedMessages.length, totalCharacters) &&
    combinedMessages.length > SUMMARY_TAIL_MESSAGE_COUNT

  if (needsSummarisation) {
    const sliceIndex = Math.max(combinedMessages.length - SUMMARY_TAIL_MESSAGE_COUNT, 0)
    const messagesToSummarise = combinedMessages.slice(0, sliceIndex)
    const messagesToKeep = combinedMessages.slice(sliceIndex)

    if (messagesToSummarise.length > 0) {
      const summaryResult = await summariseConversation({
        priorSummary: baseState.summary,
        messages: messagesToSummarise
      })

      if (summaryResult.summary) {
        workingSummary = summaryResult.summary
      }

      if (summaryResult.updated && summaryResult.summary) {
        workingMessages = messagesToKeep
        console.info('[AI] Rolled conversation into summary', {
          retainedMessages: workingMessages.length,
          summarisedCount: messagesToSummarise.length
        })
      } else if (combinedMessages.length > MAX_MESSAGE_COUNT) {
        workingMessages = combinedMessages.slice(-MAX_MESSAGE_COUNT)
        console.warn('[AI] Summary unchanged; trimming conversation to last turns', {
          retainedMessages: workingMessages.length
        })
      }
    }
  } else if (combinedMessages.length > MAX_MESSAGE_COUNT) {
    workingMessages = combinedMessages.slice(-MAX_MESSAGE_COUNT)
  }

  let messagesForRequest = buildMessagesForRequest(workingSummary, workingMessages)

  if (messagesForRequest.length > MAX_MESSAGE_COUNT) {
    const systemMessages = messagesForRequest.filter(message => message.role === 'system')
    const nonSystemMessages = messagesForRequest.filter(message => message.role !== 'system')
    const keptSystemMessages = systemMessages.slice(-1)
    const allowedNonSystemCount = Math.max(MAX_MESSAGE_COUNT - keptSystemMessages.length, 0)
    messagesForRequest = [
      ...keptSystemMessages,
      ...nonSystemMessages.slice(-allowedNonSystemCount)
    ]
  }

  const response = await executeChat({
    messages: messagesForRequest,
    currentLocation
  })

  const sanitisedMessage = sanitiseAssistantMessage(response.message)
  const routeDescriptionFallback = response.functionCall?.arguments.routeDescription
  const defaultNavigationNotice = response.functionCall
    ? `Navigation command issued for ${response.functionCall.arguments.photoId}.`
    : null
  const assistantContent = sanitiseAssistantMessage(
    sanitisedMessage ?? routeDescriptionFallback ?? defaultNavigationNotice
  )

  let nextStateMessages = workingMessages

  if (assistantContent) {
    nextStateMessages = [
      ...nextStateMessages,
      {
        role: 'assistant',
        content: assistantContent.trim()
      }
    ]
  }

  if (nextStateMessages.length > MAX_MESSAGE_COUNT) {
    nextStateMessages = nextStateMessages.slice(-MAX_MESSAGE_COUNT)
  }

  return {
    response: {
      ...response,
      message: sanitisedMessage
    },
    state: {
      summary: workingSummary,
      messages: nextStateMessages
    }
  }
}
